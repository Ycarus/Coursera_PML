error.catching
help()
q()
install.packages("tidyverse")
con <- url("http://www.uap.asia", "r")
x <- readLines(con)
head(x)
x <- 1
print(x)
x
msg <- "hello"
x <-
rm(list=ls())
a <-    # Incomplete Expression
5
?dget
help("read.csv")
?write.table
help("read.csv")
.day <- "Monday"
!day <- "Tuesday"
4 != 5
names(.day)
x = 5
names(5)
attributes(x)
x <- c(1, 3, 5, 7)
x
attributes(x)
?attributes
class(x)
y <- c("1", 5, 9)
class(y)
y
?attributes
?unclass
x <- data.frame(foo = 1:4, bar = c(T,T))
X
x
x <- data.frame(foo = 1:4, bar = c(T,T,F))
x <- data.frame(foo = 1:4, bar = c(T,F))
X
x
attributes(x)
class(x)
row.names(x)
m <- matrix(1:6, nrow = 2, ncol = 3)
m
attributes(m)
x <- list(1, "a", TRUE)
class(m)
class(x)
attributes(x)
x
x$1
x[1]
x[[1]]
x <- list(1,2,"a","b","c",TRUE)
X
x
x1 <- c(1,2,3)
y2 <- c("a","b","c")
z <- c(x1,y2)
z
a <- list(x1,y2)
a
a[[1]][3]
a[[1]]
a[[1]][2]
a[1]
class(x)
length(x)
dimensions(x)
dim(x)
names(x)
x
dim(x) <- c(1,6)
dim(x)
package.install("swirl")
swirl
library(swirl)
swirl()
my_seq <- seq(5,10,length=30)
length(my_seq)
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0, times = 40)
rep(c(0, 1, 2), times = 10)
rep(c(0, 1, 2))
rep(c(0, 1, 2), each = 10)
rm(list=ls())
n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 +x +x2 + x3 + rnorm(n, sd=.1)
ey = resid(lm(y ~ x2 + x3))
ex = resid(lm(x ~ x2 + x3))
sum(ey * ex) / sum(ex^2)
coef(lm(ey ~ ex - 1))
coef(lm(y ~ x + x2 + x3))
library(UsingR)
data("diamond")
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit$sigma)
summary(fit)$sigma
sum(resid(fit)^2)/n - 2
sqrt(sum(resid(fit)^2)/n - 2)
sqrt(sum(resid(fit)^2)/ (n - 2))
library(UsingR)
data("diamond")
x <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
y <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
fit <- lm(y ~ x)
resid(fit)
e<-resid(fit)
sd(e)
sd(e)^2
summary(fit)$sigma
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit<-lm(y~x)
influence.measures(fit)
x <- c(0.8, 0.45, 0.50, 0.73, 0.36, 0.58, 0.60, 1.85, 0.44, 1.42)
mean(x)
data("InsectSprays")
mdl4 <- glm(count ~ spray -1, family = "poisson", data = InsectSprays)
summary(mdl4)$coef
coefs <- exp(coef(mdl4))
coefs
14.5/15.33333333
14.5/2.083333333
x <- c(0.8, 0.45, 0.51, 0.73, 0.36, 0.58, 0.60, 0.85, 0.44, 0.42)
y <- c(1.40, 0.72, 1.50, 0.50, 1.20, -1.60, 1.23, -0.65, 1.49, 0.05)
lm(y ~ x)
input <- as.Date("1970-01-01")
class(input)
unclass(input)
input + 1
rm(list=ls())
# INSTALLATION
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
# PREPARING THE DATA
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
#one-hot encode
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
# DEFINING THE MODEL
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# TRAINING AND EVALUATION
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
# EVALUATE THE PERFORMANCE OF THE MODEL ON TEST DATA
model %>% evaluate(x_test, y_test)
# GENERATE PREDICTIONS ON NEW DATA
model %>% predict_classes(x_test)
# INSTALLATION
devtools::install_github("rstudio/keras")
# PREPARING THE DATA
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
#one-hot encode
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
# DEFINING THE MODEL
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# TRAINING AND EVALUATION
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
# EVALUATE THE PERFORMANCE OF THE MODEL ON TEST DATA
model %>% evaluate(x_test, y_test)
# GENERATE PREDICTIONS ON NEW DATA
model %>% predict_classes(x_test)
swirl()
swirl()
library(swirl)
swirl()
my_vector <-c(1:20)
my_vector <-(1:20)
infor()
info()
my_vector
my_vector <- 1:20
my_vector
library(swirl)
swirl()
swirl::install_course("Getting and Cleaning Data")
swirl()
factorial <- function(n)
{
if (n == 0) {
return 1
}
else {
return n*factorial(n-1)
}
}
factorial <- function(n)
{
if (n == 0) {
factorial = 1
}
else {
factorial = n*factorial(n-1)
}
}
factorial(4)
factorial(0)
View(factorial)
View(factorial)
factorial <- function(n)
{
if (n == 0) {
factorial = 1
}
else {
factorial = n*factorial(n-1)
}
}
recur_factorial <- function(n) {
if(n <= 1) {
return(1)
} else {
return(n * recur_factorial(n-1))
}
}
recur_factorial(5)
det(x)
function det()
det
rm(list=ls())
swirl()
library(swirl)
swirl()
exit
exit()
quit
quit()
rm(list=ls())
swirl::install_course("Exploratory Data Analysis")
swirl()
library(swirl)
swirl()
if (T | F) | (T & F) {
print (“eat”)
else {
print(“bulaga”)
}
if ((T | F) | (T & F)) {
print (“eat”)
else {
print(“bulaga”)
}
if ((T | F) | (T & F)) {
print ("eat")
}
else {
print("bulaga")
}
if ((T | F) | (T & F)) {
print ("eat")
} else {
print("bulaga")
}
if ((T & F) | (T & F)) {
print ("eat")
} else {
print("bulaga")
}
my_num <- c(2,4,6,8)
if (mean(my_num) != 5 | max(my_num)==8) {
print ("eat")
} else {
print("bulaga")
}
for (i in 1:4)
i  <- i+1
print(i)
for (i in 1:4)
i  <- 2i
print(i)
for (i in 1:4)
i  <- 2*i
print(i)
if ((T & F) | (T & F)) {
print ("eat")
} else {
print("bulaga")
}
if ((T | F) & (T & F)) {
print ("eat")
} else {
print("bulaga")
}
my_num <- c(2,4,6,8)
if (mean(my_num) != 5 | max(my_num)==8) {
print ("eat")
} else {
print("bulaga")
}
my_num <- c(2,4,6,8)
if (mean(my_num) != 5 & max(my_num)==8) {
print ("eat")
} else {
print("bulaga")
}
swirl()
library(swirl)
swirl()
TRUE == TRUE
(FALSE == TRUE) == FALSE
6 == 7
6 < 7
10 <= 10
5 != 7
NOT (5 == 7)
5 != 7
!(5 == 7)
FALSE & FALSE
TRUE & c(TRUE, FALSE, FALSE)
RUE && c(TRUE, FALSE, FALSE)
TRUE && c(TRUE, FALSE, FALSE)
TRUE | c(TRUE, FALSE, FALSE)
TRUE || c(TRUE, FALSE, FALSE)
5 > 8 || 6 != 8 && 4 > 3.9
isTRUE(6 > 4)
identical('twins', 'twins')
xor(5 == 6, !FALSE)
ints <- sample(10)
ints
ints > 5
which(ints > 7)
any(ints < 0)
all(ints > 0)
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags,class)
cls_list
class(cls_list)
as.character(cls_list)
?sapply
sapply(flags, class)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flags_colors <- flags[,11:17]
flag_colors <- flags[, 11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
{
for(j in 1:2)
{
print(i-j);
}
}
flag_shapes <- flags[, 19:23]
library(swirl)
swirl()
library(lattice)
mydf<-read.csv(path2csv,stringsAsFactors = FALSE)
dim(mydf)
?read.csv
library(kernlab); data(spam); set.seed(333)
rm(list=ls())
data(spam)
smallSpam <- spam[sample(dim(dim(spam)[1].size=10),]
smallSpam <- spam[sample(dim(dim(spam)[1],size=10),]
smallSpam <- spam[sample(dim(spam)[1],size=10),]
spamLabel <- (smallSpam$type="spam")*1+1
spamLabel <- (smallSpam$type=="spam")*1+1
spamLabel
plot(smallSpam$capitalAve,col=spamLabel)
plot(smallSpam$capitalAve,col=spamLabel)
library(ISLR); library(ggplot2); library(caret)
install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
library(caret)
install.packages("caret",
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
library(caret)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x=training[,c("age", "education", "jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,date=training)
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training)
qq <- qplot(age,wage,color=education,data=training)
qq + geom_smooth(method='lm',formula=y~x)
install.packages(Hmisc)
install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
p1 <- qplot(cutWage,age, data=training,fill=cutWage,geom=c("boxplot"))
p1
p2 <- qplot(cutWage,age, data=training,fill=cutWage,geom=c("boxplot","jitter"))
p2
grid.arrange(p1,p2,ncol=2)
library(ggplot2)
grid.arrange(p1,p2,ncol=2)
grid.arrange(p1,p2,ncol=2)
t1 <- table(cutWage,training$jobclass)
t1
prop.table(t1,1)
t1
qplot(wage, color=education, data=training, geom = "density")
library(caret);library(kernlab);data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab = "ave. capital run length")
rm(list=ls())
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training);dim(testing)
qplot(Petal.Width, Sepal.Width, colour=Species,data=training)
?mode
mode(iris$Sepal.Length)
modFit <- train(Species ~ . method="rpart",data=training)
library(caret)
modFit <- train(Species ~ . method="rpart",data=training)
modFit <- train(Species ~ ., method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main="Classification Tree")
text(modFit$finalModel, use.n = TRUE, all=TRUE, cex=.8)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=testing)
library(ElemStatLearn); data(ozone,package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone)]
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA,nrow=10,ncol=155)
for (i in 1:10) {}
for (i in 1:10) { }
for (i in 1:10) {ss <-sa
}
rm(list=ls())
data(iris);library(ggplot2)
inTrain <-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
modFit <- train(Species~.,data=training, method="rf",prox=TRUE)
modFit
rm(list=ls())
setwd("~/Downloads/Practical Machine Learning/project")
